---
title: "Kubernetesä¸Šéƒ¨ç½²vLLM"
date: 2025-12-01
description: "vLLMæ˜¯ä¸€ä¸ªä¸“ä¸ºå¤§è¯­è¨€æ¨¡å‹æ¨ç†è®¾è®¡çš„é«˜æ€§èƒ½æœåŠ¡æ¡†æ¶ï¼Œå…¶æ ¸å¿ƒä¼˜åŠ¿åœ¨äºåˆ›æ–°çš„PagedAttentionæŠ€æœ¯ï¼Œèƒ½å¤Ÿæ˜¾è‘—æå‡GPUå†…å­˜åˆ©ç”¨ç‡å’Œæ¨ç†ååé‡ã€‚é€šè¿‡Dockerå®¹å™¨åŒ–å°è£…ï¼ŒvLLMå®ç°äº†ç¯å¢ƒæ ‡å‡†åŒ–å’Œä¾èµ–éš”ç¦»ï¼Œè€ŒKuberneteséƒ¨ç½²åˆ™è¿›ä¸€æ­¥å¸¦æ¥äº†..."
---
## 1. vLLM Dockeré•œåƒä¸Kuberneteséƒ¨ç½²ä»·å€¼

vLLMæ˜¯ä¸€ä¸ªä¸“ä¸ºå¤§è¯­è¨€æ¨¡å‹æ¨ç†è®¾è®¡çš„é«˜æ€§èƒ½æœåŠ¡æ¡†æ¶ï¼Œå…¶æ ¸å¿ƒä¼˜åŠ¿åœ¨äºåˆ›æ–°çš„PagedAttentionæŠ€æœ¯ï¼Œèƒ½å¤Ÿæ˜¾è‘—æå‡GPUå†…å­˜åˆ©ç”¨ç‡å’Œæ¨ç†ååé‡ã€‚é€šè¿‡Dockerå®¹å™¨åŒ–å°è£…ï¼ŒvLLMå®ç°äº†ç¯å¢ƒæ ‡å‡†åŒ–å’Œä¾èµ–éš”ç¦»ï¼Œè€ŒKuberneteséƒ¨ç½²åˆ™è¿›ä¸€æ­¥å¸¦æ¥äº†ï¼š

- **å¼¹æ€§ä¼¸ç¼©**ï¼šæ ¹æ®è´Ÿè½½è‡ªåŠ¨è°ƒæ•´å‰¯æœ¬æ•°é‡
- **èµ„æºéš”ç¦»**ï¼šGPUèµ„æºçš„ç²¾ç»†åŒ–ç®¡ç†å’Œéš”ç¦»
- **é«˜å¯ç”¨æ€§**ï¼šè‡ªåŠ¨æ•…éšœæ¢å¤å’Œè´Ÿè½½å‡è¡¡
- **ç®€åŒ–è¿ç»´**ï¼šç»Ÿä¸€çš„éƒ¨ç½²ã€ç›‘æ§å’Œç®¡ç†ç•Œé¢

vLLMå®˜æ–¹Dockeré•œåƒæä¾›äº†å¼€ç®±å³ç”¨çš„æ¨¡å‹æœåŠ¡ç¯å¢ƒï¼Œç»“åˆKubernetesçš„ç¼–æ’èƒ½åŠ›ï¼Œä¸ºç”Ÿäº§çº§AIæœåŠ¡æä¾›äº†åšå®åŸºç¡€ã€‚

## 2. Qwen3-235B-A22B-Instruct-2507æ¨¡å‹éƒ¨ç½²å®è·µ

### 2.1 ä»ModelScopeä¸‹è½½æ¨¡å‹

Qwen3-235B-A22B-Instruct-2507ä½œä¸ºåƒé—®ç³»åˆ—çš„æœ€æ–°å¤§æ¨¡å‹ï¼Œé¦–å…ˆéœ€è¦ä»ModelSpaceè·å–æ¨¡å‹æƒé‡ï¼š

```python
# ä½¿ç”¨modelscopeåº“ä¸‹è½½æ¨¡å‹
from modelscope import snapshot_download
model_dir = snapshot_download(
    'Qwen/Qwen3-235B-A22B-Instruct-2507',
    cache_dir='/workspace/models',
    revision='v1.0.0'
)
```

å¯¹äºKubernetesç¯å¢ƒï¼Œæ¨èä½¿ç”¨åˆå§‹åŒ–å®¹å™¨è¿›è¡Œæ¨¡å‹ä¸‹è½½ï¼Œç¡®ä¿æ¨¡å‹æ–‡ä»¶åœ¨Podå¯åŠ¨å‰å‡†å¤‡å°±ç»ªã€‚

## 3. vLLMæœåŠ¡é…ç½®ä¸Kuberneteséƒ¨ç½²

### 3.1 vLLMå¯åŠ¨å‚æ•°ä¼˜åŒ–

é’ˆå¯¹Qwen3-235Bå¤§æ¨¡å‹ï¼ŒvLLMéœ€è¦ç‰¹å®šé…ç½®ä»¥å……åˆ†å‘æŒ¥æ€§èƒ½ï¼š

```bash
vllm serve Qwen/Qwen3-235B-A22B-Instruct-2507 \
    --tensor-parallel-size 8 \
    --gpu-memory-utilization 0.85 \
    --max-model-len 131072 \
    --served-model-name qwen3-235b \
    --port 9997 \
    --host 0.0.0.0 \
    --trust-remote-code \
    --dtype auto \
    --enable-prefix-caching \
    --enable-chunked-prefill \
    --api-key sk-xxxxxxxxx \
    --tool-call-parser hermes \
    --enable-auto-tool-choice \
    --swap-space 16 \
    --disable-log-requests
```

å…³é”®å‚æ•°è¯´æ˜ï¼š

- tensor-parallel-size 8ï¼š8è·¯å¼ é‡å¹¶è¡Œï¼Œå……åˆ†åˆ©ç”¨å¤šGPUèµ„æº
- gpu-memory-utilization 0.85ï¼šé€‚ä¸­çš„GPUå†…å­˜åˆ©ç”¨ç‡ï¼Œé¢„ç•™ç¼“å†²ç©ºé—´
- max-model-len 131072ï¼šæ”¯æŒ128Kä¸Šä¸‹æ–‡é•¿åº¦
- enable-prefix-cachingï¼šå¯ç”¨å‰ç¼€ç¼“å­˜ï¼Œæå‡æ¨ç†æ•ˆç‡
- enable-chunked-prefillï¼šåˆ†å—é¢„å¡«å……ï¼Œä¼˜åŒ–é•¿æ–‡æœ¬å¤„ç†
- swap-space 16ï¼š16GBäº¤æ¢ç©ºé—´ï¼Œå¤„ç†å†…å­˜æº¢å‡º

### 3.2 Kubernetes Deploymenté…ç½®

```yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  labels:
    app: vllm-qwen3-235b
    nvidia.com/app: vllm-qwen3-235b
    nvidia.com/framework: python
    nvidia.com/unit: application
  name: vllm-qwen3-235b
  namespace: ai-serving
spec:
  podManagementPolicy: OrderedReady
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app: vllm-qwen3-235b
  serviceName: vllm-qwen3-235b
  template:
    metadata:
      annotations:
        nvidia.com/use-gputype: A100,A800,H100,H800
        nvidia.com/globalid: AIP_MDIS_QWEN3_235B
        nvidia.com/gpu: "8"
        nvidia.com/gpumem: "638976"  # 8 * 79872
        nvidia.com/model-uid: Qwen3-235B-A22B-Instruct-2507
      labels:
        app: vllm-qwen3-235b
        type: llm
        model-size: 235b
        nvidia.com/app: vllm-qwen3-235b
        nvidia.com/framework: python
        nvidia.com/unit: application
        nvidia.com/4pd-scheduler: "true"
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: accelerator
                operator: In
                values: ["nvidia-a100-80gb", "nvidia-a800-80gb", "nvidia-h100-80gb"]
              - key: topology.kubernetes.io/zone
                operator: In
                values: ["zone-gpu-high"]
      initContainers:
      - name: download-model
        image: download-model:latest
        imagePullPolicy: IfNotPresent
        command: ["sh", "-c"]
        args:
        - |
          echo "å¼€å§‹ä¸‹è½½Qwen3-235Bæ¨¡å‹..."
          model_download /opt/app/models/ || {
            echo "æ¨¡å‹ä¸‹è½½å¤±è´¥ï¼Œå°è¯•ä»å¤‡ç”¨æºä¸‹è½½..."
            exit 1
          }
          echo "æ¨¡å‹ä¸‹è½½å®Œæˆï¼ŒéªŒè¯æ¨¡å‹æ–‡ä»¶..."
          ls -la /opt/app/models/
          if [ ! -f "/opt/app/models/config.json" ]; then
            echo "æ¨¡å‹æ–‡ä»¶éªŒè¯å¤±è´¥"
            exit 1
          fi
          echo "æ¨¡å‹æ–‡ä»¶éªŒè¯æˆåŠŸ"
        env:
        - name: S3_URL
          valueFrom:
            secretKeyRef:
              key: S3_URL
              name: vllm-qwen3-235b
        - name: S3_AK
          valueFrom:
            secretKeyRef:
              key: S3_AK
              name: vllm-qwen3-235b
        - name: S3_SK
          valueFrom:
            secretKeyRef:
              key: S3_SK
              name: vllm-qwen3-235b
        - name: S3_BUCKET
          valueFrom:
            secretKeyRef:
              key: S3_BUCKET
              name: vllm-qwen3-235b
        - name: USERNAME
          value: "ai-platform"
        - name: MODEL_NAME
          value: "qwen3-235b"
        - name: VERSION
          value: "Qwen3-235B-A22B-Instruct-2507"
        resources:
          requests:
            cpu: "2"
            memory: "4Gi"
          limits:
            cpu: "4"
            memory: "8Gi"
        volumeMounts:
        - mountPath: /opt/app/models
          name: share-volume
      containers:
      - name: vllm-qwen3-235b
        image: vllm/vllm-openai:v0.12.0
        imagePullPolicy: IfNotPresent
        command: ["/bin/bash"]
        args:
        - -c
        - |
          set -e
          
          # ç¯å¢ƒå˜é‡è®¾ç½®
          export NCCL_SHM_DISABLE=1
          export VLLM_ALLOW_LONG_MAX_MODEL_LEN=1
          export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
          export NCCL_IB_DISABLE=0
          export NCCL_NET_GDR_LEVEL=2
          
          # åˆ›å»ºæ—¥å¿—ç›®å½•
          mkdir -p /log/logs
          
          # ç­‰å¾…æ¨¡å‹æ–‡ä»¶å°±ç»ª
          while [ ! -f "/opt/app/models/config.json" ]; do
            echo "ç­‰å¾…æ¨¡å‹æ–‡ä»¶å°±ç»ª..."
            sleep 10
          done
          
          echo "å¯åŠ¨vLLMæœåŠ¡..."
          exec python3 -m vllm.entrypoints.openai.api_server \
            --model=/opt/app/models \
            --served-model-name=Qwen3-235B-A22B-Instruct-2507 \
            --port=9997 \
            --host=0.0.0.0 \
            --trust-remote-code \
            --dtype=auto \
            --enable-prefix-caching \
            --enable-chunked-prefill \
            --tensor-parallel-size=8 \
            --gpu-memory-utilization=0.85 \
            --api-key=${API_KEY} \
            --max-model-len=131072 \
            --tool-call-parser=hermes \
            --enable-auto-tool-choice \
            --swap-space=16 \
            --disable-log-requests \
            --max-num-seqs=128 \
            --max-paddings=256 \
            > /log/logs/vllm-qwen3-235b.log 2>&1
        env:
        - name: ACTIVE_OOM_KILLER
          value: "0"
        - name: LIBCUDA_LOG_LEVEL
          value: "0"
        - name: GPU_CORE_UTILIZATION_POLICY
          value: "disable"
        - name: VLLM_WORKER_MULTIPROC_METHOD
          value: "spawn"
        - name: VLLM_LOGGING_LEVEL
          value: "INFO"
        - name: MODEL_UID
          value: "Qwen3-235B-A22B-Instruct-2507"
        - name: MODEL_TYPE
          value: "chat"
        - name: API_KEY
          valueFrom:
            secretKeyRef:
              key: api-key
              name: vllm-qwen3-235b
        - name: NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: POD_IP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        lifecycle:
          preStop:
            exec:
              command:
              - /bin/bash
              - -c
              - |
                echo "æ­£åœ¨ä¼˜é›…å…³é—­vLLMæœåŠ¡..."
                kill -TERM $(pgrep -f "vllm.entrypoints.openai.api_server") || true
                sleep 30
        ports:
        - containerPort: 9997
          name: api-port
          protocol: TCP
        livenessProbe:
          httpGet:
            path: /health
            port: 9997
            scheme: HTTP
          failureThreshold: 3
          initialDelaySeconds: 600
          periodSeconds: 60
          successThreshold: 1
          timeoutSeconds: 30
        readinessProbe:
          httpGet:
            path: /health
            port: 9997
            scheme: HTTP
          failureThreshold: 3
          initialDelaySeconds: 300
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 10
        startupProbe:
          httpGet:
            path: /health
            port: 9997
            scheme: HTTP
          failureThreshold: 30
          initialDelaySeconds: 120
          periodSeconds: 30
          successThreshold: 1
          timeoutSeconds: 10
        resources:
          requests:
            cpu: "16"
            memory: "64Gi"
            nvidia.com/gpu: "8"
            nvidia.com/gpumem: "638976"
          limits:
            cpu: "64"
            memory: "256Gi"
            nvidia.com/gpu: "8"
            nvidia.com/gpumem: "638976"
        volumeMounts:
        - mountPath: /opt/app/models
          name: share-volume
          readOnly: true
        - mountPath: /dev/shm
          name: cache-volume
        - mountPath: /etc/localtime
          name: timezone-volume
          readOnly: true
        - mountPath: /log
          name: log-volume
        - mountPath: /tmp
          name: temp-volume
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext:
        runAsNonRoot: false
        fsGroup: 0
      terminationGracePeriodSeconds: 180
      tolerations:
      - effect: NoSchedule
        key: role
        operator: Equal
        value: bigdata
      - effect: NoSchedule
        key: gpurole
        operator: Equal
        value: gpu
      - effect: NoSchedule
        key: model-size
        operator: Equal
        value: large
      volumes:
      - name: share-volume
        persistentVolumeClaim:
          claimName: vllm-qwen3-235b-model-pvc
      - name: cache-volume
        emptyDir:
          medium: Memory
          sizeLimit: 256Gi
      - name: log-volume
        hostPath:
          path: /data/logs/vllm-qwen3-235b
          type: DirectoryOrCreate
      - name: temp-volume
        emptyDir:
          sizeLimit: 32Gi
      - name: timezone-volume
        hostPath:
          path: /usr/share/zoneinfo/Asia/Shanghai
          type: FileOrCreate
  updateStrategy:
    type: RollingUpdate
    rollingUpdate:
      partition: 0
  volumeClaimTemplates: []

---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: vllm-qwen3-235b-model-pvc
  namespace: ai-serving
  labels:
    app: vllm-qwen3-235b
spec:
  accessModes:
  - ReadWriteMany
  resources:
    requests:
      storage: 1Ti
  storageClassName: nfs-client

---
apiVersion: v1
kind: Service
metadata:
  name: vllm-qwen3-235b
  namespace: ai-serving
  labels:
    app: vllm-qwen3-235b
spec:
  type: ClusterIP
  ports:
  - port: 9997
    targetPort: 9997
    protocol: TCP
    name: api-port
  selector:
    app: vllm-qwen3-235b

---
apiVersion: v1
kind: Secret
metadata:
  name: vllm-qwen3-235b
  namespace: ai-serving
type: Opaque
data:
  api-key: c2stdmxsbS1xd2VuMy0yMzViLWFwaS1rZXk=  # base64ç¼–ç çš„API key
  S3_URL: <base64-encoded-s3-url>
  S3_AK: <base64-encoded-access-key>
  S3_SK: <base64-encoded-secret-key>
  S3_BUCKET: <base64-encoded-bucket-name>
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: vllm-qwen3-235b-model-pvc
  namespace: ai-serving
  labels:
    app: vllm-qwen3-235b
spec:
  accessModes:
  - ReadWriteMany
  resources:
    requests:
      storage: 1Ti
  storageClassName: nfs-client
```

### 3.3 å­˜å‚¨ä¸æ¨¡å‹ç®¡ç†

é€šè¿‡PVCç”³è¯·å…±äº«å­˜å‚¨ï¼Œç¡®ä¿æ¨¡å‹æ–‡ä»¶æŒä¹…åŒ–å¹¶å¯åœ¨å¤šä¸ªPodé—´å…±äº«ã€‚initContainerè´Ÿè´£ä»S3å¯¹è±¡å­˜å‚¨ä¸‹è½½æ¨¡å‹åˆ°æŒä¹…åŒ–å­˜å‚¨ï¼Œé¿å…æ¯æ¬¡Podé‡å¯é‡å¤ä¸‹è½½ã€‚

## 4. Hami on K8Sæ¶æ„çš„GPUèµ„æºåŠ¨æ€åˆ†é…
![alt text](hami-horizontal-colordark.png)
### 4.1 HAMiç®€ä»‹ä¸ä¼˜åŠ¿

**HAMiï¼ˆHeterogeneous AI Computing Virtualization Middlewareï¼‰** æ˜¯ä¸€ä¸ªä¸“ä¸ºå¼‚æ„AIè®¡ç®—è®¾å¤‡è®¾è®¡çš„è™šæ‹ŸåŒ–ä¸­é—´ä»¶ï¼Œä¸ºKubernetesé›†ç¾¤æä¾›å¼ºå¤§çš„GPUèµ„æºç®¡ç†èƒ½åŠ›ã€‚


#### æ ¸å¿ƒä¼˜åŠ¿

- **ğŸ”„ è®¾å¤‡è™šæ‹ŸåŒ–**ï¼šä¸ºå¤šç§å¼‚æ„è®¾å¤‡æä¾›è™šæ‹ŸåŒ–åŠŸèƒ½ï¼Œæ”¯æŒè®¾å¤‡å…±äº«å’Œèµ„æºéš”ç¦»
- **ğŸš€ æ™ºèƒ½è°ƒåº¦**ï¼šåŸºäºè®¾å¤‡æ‹“æ‰‘å’Œè°ƒåº¦ç­–ç•¥å®ç°Podé—´çš„è®¾å¤‡å…±äº«å’Œä¼˜åŒ–è°ƒåº¦
- **ğŸ¯ ç»Ÿä¸€ç®¡ç†**ï¼šæ¶ˆé™¤ä¸åŒå¼‚æ„è®¾å¤‡é—´çš„å·®å¼‚ï¼Œæä¾›ç»Ÿä¸€ç®¡ç†æ¥å£
- **ğŸ’¡ é›¶ä¿®æ”¹éƒ¨ç½²**ï¼šæ— éœ€å¯¹ç°æœ‰åº”ç”¨ç¨‹åºè¿›è¡Œä»»ä½•ä¿®æ”¹

![alt text](image.png)
### 4.2 æ”¯æŒçš„å¼‚æ„è®¾å¤‡

HAMi 2.7.1ç‰ˆæœ¬æ”¯æŒçš„è®¾å¤‡ç±»å‹å¦‚ä¸‹ï¼š

| è®¾å¤‡å‚å•† | è®¾å¤‡ç±»å‹ | æ”¯æŒçŠ¶æ€ | å¤‡æ³¨ |
|---------|----------|----------|------|
| **NVIDIA** | GPU | âœ… å®Œæ•´æ”¯æŒ | éœ€è¦é©±åŠ¨ >= 440 |
| **å¯’æ­¦çºª** | MLU | âœ… å®Œæ•´æ”¯æŒ | - |
| **æµ·å…‰** | DCU | âœ… å®Œæ•´æ”¯æŒ | - |
| **å¤©æ•°æ™ºèŠ¯** | GPU | âœ… å®Œæ•´æ”¯æŒ | - |
| **æ‘©å°”çº¿ç¨‹** | GPU | âœ… å®Œæ•´æ”¯æŒ | - |
| **æ˜‡è…¾** | NPU | âœ… å®Œæ•´æ”¯æŒ | - |
| **æ²æ›¦** | GPU | âœ… å®Œæ•´æ”¯æŒ | - |

> ğŸ“ˆ **æ”¯æŒè®¾å¤‡æŒç»­æ›´æ–°ä¸­**ï¼Œæœ€æ–°æ”¯æŒåˆ—è¡¨è¯·æŸ¥çœ‹ [å®˜æ–¹æ–‡æ¡£](https://github.com/Project-HAMi/HAMi#preparing-your-gpu-nodes)

### 4.3 HAMiå®‰è£…éƒ¨ç½²

#### 4.3.1 å‰ç½®æ¡ä»¶æ£€æŸ¥

åœ¨å®‰è£…HAMiä¹‹å‰ï¼Œè¯·ç¡®ä¿æ»¡è¶³ä»¥ä¸‹ç³»ç»Ÿè¦æ±‚ï¼š

| ç»„ä»¶ç±»åˆ« | ç»„ä»¶åç§° | ç‰ˆæœ¬è¦æ±‚ | è¯¦ç»†è¯´æ˜ |
|---------|----------|----------|----------|
| **GPUé©±åŠ¨** | NVIDIAé©±åŠ¨ | â‰¥ 440.x | â€¢ GPUè®¡ç®—èŠ‚ç‚¹å¿…éœ€å®‰è£…<br>â€¢ å»ºè®®ä½¿ç”¨æœ€æ–°ç¨³å®šç‰ˆæœ¬<br>â€¢ æ”¯æŒCUDA 11.0+ |
| **å®¹å™¨è¿è¡Œæ—¶** | nvidia-docker | â‰¥ 2.0 | â€¢ GPUå®¹å™¨åŒ–æ”¯æŒç»„ä»¶<br>â€¢ å¿…é¡»é…ç½®ä¸ºé»˜è®¤è¿è¡Œæ—¶<br>â€¢ ä¸Docker Engineé…åˆä½¿ç”¨ |
| **å®¹å™¨ç¼–æ’** | Kubernetes | â‰¥ 1.18 | â€¢ é›†ç¾¤ç®¡ç†å¹³å°<br>â€¢ å»ºè®®1.20+ç‰ˆæœ¬ä»¥è·å¾—æ›´å¥½çš„GPUè°ƒåº¦æ”¯æŒ<br>â€¢ éœ€å¯ç”¨GPUè®¾å¤‡æ’ä»¶ |
| **å®¹å™¨å¼•æ“** | å®¹å™¨è¿è¡Œæ—¶ | å…¼å®¹ç‰ˆæœ¬ | â€¢ æ”¯æŒï¼šcontainerd/docker/cri-o<br>â€¢ å¿…é¡»é…ç½®nvidiaä¸ºé»˜è®¤è¿è¡Œæ—¶<br>â€¢ ç¡®ä¿ä¸K8sç‰ˆæœ¬å…¼å®¹ |
| **ç³»ç»Ÿåº“** | glibc | â‰¥ 2.17 ä¸” < 2.30 | â€¢ GNU Cæ ‡å‡†åº“<br>â€¢ ç‰ˆæœ¬èŒƒå›´ä¸¥æ ¼é™åˆ¶<br>â€¢ å½±å“CUDAåº“å…¼å®¹æ€§ |
| **æ“ä½œç³»ç»Ÿ** | Linuxå†…æ ¸ | â‰¥ 3.10 | â€¢ æœ€ä½å†…æ ¸ç‰ˆæœ¬è¦æ±‚<br>â€¢ å»ºè®®ä½¿ç”¨4.x+å†…æ ¸<br>â€¢ ç¡®ä¿é©±åŠ¨å…¼å®¹æ€§ |
| **åŒ…ç®¡ç†** | Helm | â‰¥ 3.0 | â€¢ KubernetesåŒ…ç®¡ç†å·¥å…·<br>â€¢ ç”¨äºéƒ¨ç½²GPU Operator<br>â€¢ å»ºè®®ä½¿ç”¨æœ€æ–°ç¨³å®šç‰ˆ |

> âš ï¸ **é‡è¦**ï¼šç¡®ä¿å®¹å™¨è¿è¡Œæ—¶çš„é»˜è®¤è¿è¡Œæ—¶é…ç½®ä¸ºnvidia

#### 4.3.2 å®‰è£…æ­¥éª¤

**Step 1: æ ‡è®°GPUèŠ‚ç‚¹**

ä¸ºéœ€è¦GPUè°ƒåº¦çš„èŠ‚ç‚¹æ·»åŠ æ ‡ç­¾ï¼š

```bash
kubectl label nodes {nodeid} gpu=on
```

> ğŸ’¡ **æç¤º**ï¼šåªæœ‰å¸¦æœ‰ `gpu=on` æ ‡ç­¾çš„èŠ‚ç‚¹æ‰èƒ½è¢«HAMiè°ƒåº¦å™¨ç®¡ç†

**Step 2: æ·»åŠ Helmä»“åº“**

```bash
helm repo add hami-charts https://project-hami.github.io/HAMi/
helm repo update
```

**Step 3: éƒ¨ç½²HAMi**

```bash
helm install hami hami-charts/hami -n kube-system
```

**Step 4: éªŒè¯å®‰è£…**

```bash
kubectl get pods -n kube-system | grep vgpu
```

æœŸæœ›è¾“å‡ºï¼š
```
vgpu-device-plugin-xxx    1/1     Running   0          2m
vgpu-scheduler-xxx        1/1     Running   0          2m
```

> âœ… å¦‚æœ `vgpu-device-plugin` å’Œ `vgpu-scheduler` Podéƒ½å¤„äºRunningçŠ¶æ€ï¼Œåˆ™å®‰è£…æˆåŠŸ


### 4.4 è‡ªå®šä¹‰é…ç½®

HAMiæ”¯æŒé€šè¿‡ä¿®æ”¹ `values.yaml` æ–‡ä»¶è¿›è¡Œä¸ªæ€§åŒ–é…ç½®ã€‚

#### 4.4.1 èµ„æºåç§°è‡ªå®šä¹‰

```yaml
# NVIDIA GPU å‚æ•°é…ç½®
resourceName: "nvidia.com/gpu"                    # GPUè®¾å¤‡èµ„æºå
resourceMem: "nvidia.com/gpumem"                  # GPUå†…å­˜èµ„æºå
resourceMemPercentage: "nvidia.com/gpumem-percentage"  # GPUå†…å­˜ç™¾åˆ†æ¯”
resourceCores: "nvidia.com/gpucores"              # GPUæ ¸å¿ƒæ•°
resourcePriority: "nvidia.com/priority"           # GPUä¼˜å…ˆçº§
```

#### 4.4.2 ä¼ä¸šçº§è‡ªå®šä¹‰ç¤ºä¾‹

```yaml
# è‡ªå®šä¹‰ä¸ºå…¬å¸æ ‡è¯†
resourceName: "company.com/gpu"
resourceMem: "company.com/gpumem"
resourceMemPercentage: "company.com/gpumem-percentage"
resourceCores: "company.com/gpucores"
```

> ğŸ“š **æ›´å¤šé…ç½®é€‰é¡¹**ï¼šè¯¦ç»†é…ç½®å‚æ•°è¯·å‚è€ƒ [å®˜æ–¹é…ç½®æ–‡æ¡£](https://github.com/Project-HAMi/HAMi/blob/master/docs/config.md)

### 4.5 ä½¿ç”¨æ³¨æ„äº‹é¡¹

åœ¨ä½¿ç”¨HAMiè¿‡ç¨‹ä¸­ï¼Œè¯·æ³¨æ„ä»¥ä¸‹é‡è¦äº‹é¡¹ï¼š

| âš ï¸ **é‡è¦æé†’** |
|-----------------|
| **è®¾å¤‡æš´éœ²é£é™©**ï¼šå¦‚æœä½¿ç”¨NVIDIAé•œåƒçš„è®¾å¤‡æ’ä»¶æ—¶ä¸è¯·æ±‚è™šæ‹ŸGPUï¼Œæœºå™¨ä¸Šçš„æ‰€æœ‰GPUå¯èƒ½ä¼šåœ¨å®¹å™¨å†…æš´éœ² |
| **A100 MIGé™åˆ¶**ï¼šç›®å‰A100 MIGä»…æ”¯æŒ "none" å’Œ "mixed" æ¨¡å¼ |
| **è°ƒåº¦çº¦æŸ**ï¼šå¸¦æœ‰ "nodeName" å­—æ®µçš„ä»»åŠ¡ç›®å‰æ— æ³•è°ƒåº¦ï¼Œè¯·ä½¿ç”¨ "nodeSelector" ä»£æ›¿ |
| **ç‰ˆæœ¬ä¸€è‡´æ€§**ï¼šç¡®ä¿Kubernetes schedulerç‰ˆæœ¬ä¸hami-schedulerç‰ˆæœ¬ä¸€è‡´ |

### 4.6 æœ€ä½³å®è·µå»ºè®®

1. **ğŸ“‹ éƒ¨ç½²å‰æ£€æŸ¥**
   - éªŒè¯æ‰€æœ‰å‰ç½®æ¡ä»¶
   - ç¡®è®¤GPUèŠ‚ç‚¹æ ‡ç­¾æ­£ç¡®
   - æ£€æŸ¥ç‰ˆæœ¬å…¼å®¹æ€§

2. **ğŸ”§ é…ç½®ä¼˜åŒ–**
   - æ ¹æ®ä¼ä¸šéœ€æ±‚è‡ªå®šä¹‰èµ„æºåç§°
   - åˆç†é…ç½®èµ„æºé™åˆ¶
   - ç›‘æ§èµ„æºä½¿ç”¨æƒ…å†µ

3. **ğŸ›¡ï¸ å®‰å…¨è€ƒè™‘**
   - é¿å…ä¸å¿…è¦çš„GPUè®¾å¤‡æš´éœ²
   - åˆç†è®¾ç½®èµ„æºé…é¢
   - å®šæœŸæ›´æ–°ç»„ä»¶ç‰ˆæœ¬



## 5. æœåŠ¡è®¿é—®ä¸å¯æ‰©å±•æ€§æ¼”ç¤º

### 5.1 æœåŠ¡æš´éœ²ä¸è®¿é—®

é€šè¿‡Serviceå’ŒIngressæš´éœ²vLLMæœåŠ¡ï¼š

```yaml
apiVersion: v1
kind: Service
metadata:
  name: vllm-service
  namespace: ai-serving
spec:
  selector:
    app: vllm-qwen3-235b
  ports:
  - port: 9997
    targetPort: 9997
  type: ClusterIP

---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: vllm-ingress
  namespace: ai-serving
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  rules:
  - host: vllm.ai-serving.company.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: vllm-service
            port:
              number: 9997
```

### 5.2 APIè°ƒç”¨ç¤ºä¾‹
cURL è¯·æ±‚ç¤ºä¾‹
éæµå¼å“åº”ç¤ºä¾‹: 
```
curl -X POST "http://vllm.ai-serving.company.com/v1/chat/completions" \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer token-abc123" \
  -d '{
    "model": "qwen3-235b",
    "messages": [
      {
        "role": "user", 
        "content": "è§£é‡Šæœºå™¨å­¦ä¹ çš„åŸºæœ¬æ¦‚å¿µ"
      }
    ],
    "max_tokens": 1000,
    "temperature": 0.7,
    "stream": false,
    "enable_reasoning": true
  }'

```
æµå¼å“åº”ç¤ºä¾‹: 

``` 
curl -X POST "http://vllm.ai-serving.company.com/v1/chat/completions" \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer token-abc123" \
  -d '{
    "model": "qwen3-235b",
    "messages": [
      {
        "role": "user", 
        "content": "è§£é‡Šæœºå™¨å­¦ä¹ çš„åŸºæœ¬æ¦‚å¿µ"
      }
    ],
    "max_tokens": 1000,
    "temperature": 0.7,
    "stream": true,
    "enable_reasoning": false
  }' \
  --no-buffer
```
### 5.3 å¯æ‰©å±•æ€§æ¼”ç¤º

**æ°´å¹³æ‰©ç¼©å®¹æ¼”ç¤ºï¼š**

```bash
# æ‰‹åŠ¨æ‰©å±•å‰¯æœ¬æ•°åº”å¯¹æµé‡é«˜å³°
kubectl scale sts vllm-qwen3-235b --replicas=2 -n ai-serving
```

**ç›‘æ§ä¸æŒ‡æ ‡ï¼š**

- GPUåˆ©ç”¨ç‡ç›‘æ§è‡ªåŠ¨è§¦å‘æ‰©å®¹
- è¯·æ±‚å»¶è¿Ÿè¶…è¿‡é˜ˆå€¼æ—¶å¢åŠ å‰¯æœ¬
- åŸºäºPrometheusæŒ‡æ ‡çš„è‡ªå®šä¹‰æ‰©ç¼©å®¹ç­–ç•¥

## æ€»ç»“

Kubernetesä¸Šéƒ¨ç½²vLLMä¸ºå¤§è§„æ¨¡è¯­è¨€æ¨¡å‹æœåŠ¡æä¾›äº†ç”Ÿäº§çº§çš„è§£å†³æ–¹æ¡ˆã€‚é€šè¿‡å®¹å™¨åŒ–å°è£…ã€èµ„æºåŠ¨æ€è°ƒåº¦ã€å­˜å‚¨æŒä¹…åŒ–å’Œè‡ªåŠ¨æ‰©ç¼©å®¹ç­‰ç‰¹æ€§ï¼Œå®ç°äº†é«˜æ•ˆã€å¯é ä¸”å¯æ‰©å±•çš„AIæ¨¡å‹æœåŠ¡æ¶æ„ã€‚Qwen3-235Bç­‰å¤§æ¨¡å‹åœ¨è¯¥æ¶æ„ä¸‹èƒ½å¤Ÿå……åˆ†å‘æŒ¥æ€§èƒ½ä¼˜åŠ¿ï¼Œä¸ºä¼ä¸šçº§AIåº”ç”¨æä¾›å¼ºæœ‰åŠ›çš„æ”¯æ’‘ã€‚

è¿™ç§æ¶æ„ä¸ä»…é€‚ç”¨äºå½“å‰çš„å¤§æ¨¡å‹æœåŠ¡ï¼Œä¹Ÿä¸ºæœªæ¥æ›´å¤§è§„æ¨¡çš„æ¨¡å‹å’ŒæœåŠ¡éœ€æ±‚æä¾›äº†å¯æ‰©å±•çš„åŸºç¡€è®¾æ–½ä¿éšœï¼Œæ˜¯æ„å»ºç°ä»£åŒ–AIæœåŠ¡å¹³å°çš„æœ€ä½³å®è·µã€‚
